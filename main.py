from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import asyncio
import os
import time
import re
import uuid
from openai import AsyncOpenAI
import logging
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("LangAPI")

app = FastAPI(title="LangAPI", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class TranslationRequest(BaseModel):
    content: str
    sourceLanguage: str = "en"
    targetLanguage: str

class RequestLogger:
    def __init__(self, request_id: str):
        self.request_id = request_id
        self.start_time = time.time()
        self.chunk_results = []
    
    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        log_entry = f"[{self.request_id}] {timestamp} | {level} | {message}"
        logger.info(log_entry)
    
    def log_chunk_result(self, chunk_id: int, success: bool, chars: int, processing_time: float, error: str = None):
        result = {
            "chunk_id": chunk_id,
            "success": success,
            "characters": chars,
            "processing_time_ms": round(processing_time * 1000, 2),
            "error": error
        }
        self.chunk_results.append(result)
        
        status = "Success" if success else "Failed"
        self.log(f"Chunk {chunk_id}: {status} | {chars} chars | {result['processing_time_ms']}ms")
    
    def get_summary(self):
        total_time = time.time() - self.start_time
        successful_chunks = len([r for r in self.chunk_results if r['success']])
        failed_chunks = len([r for r in self.chunk_results if not r['success']])
        
        return {
            "request_id": self.request_id,
            "total_processing_time": round(total_time, 3),
            "chunks_successful": successful_chunks,
            "chunks_failed": failed_chunks,
            "chunk_details": self.chunk_results
        }

_openai_client = None

def get_openai_client():
    global _openai_client
    if _openai_client is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OpenAI API key not found in environment variables")
            return None
        try:
            _openai_client = AsyncOpenAI(
                api_key=api_key, 
                timeout=45.0,
                max_retries=1
            )
            logger.info("OpenAI client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            return None
    return _openai_client

def clean_translated_html(original_content, translated_content):
    cleaned = translated_content.strip()
    
    if not original_content.strip().startswith('<html') and cleaned.startswith('<html'):
        cleaned = re.sub(r'^<html[^>]*>(.*)</html>$', r'\1', cleaned, flags=re.DOTALL)
    if not original_content.strip().startswith('<body') and cleaned.startswith('<body'):
        cleaned = re.sub(r'^<body[^>]*>(.*)</body>$', r'\1', cleaned, flags=re.DOTALL)
    
    return cleaned.strip()

class FastHTMLChunker:
    def __init__(self):
        self.target_chars = 4000
        self.max_chunks = 15
        
    def calculate_optimal_config(self, content_length, req_logger):
        if content_length < 5000:
            config = (1, 5, "Small content")
        elif content_length < 15000:
            config = (3, 15, "Medium content")
        elif content_length < 40000:
            config = (8, 25, "Large content")
        else:
            config = (15, 30, "Very large content")
        
        max_chunks, parallel_limit, size_category = config
        req_logger.log(f"Fast mode: {content_length} chars â†’ {size_category}")
        req_logger.log(f"Config: Max {max_chunks} chunks, {parallel_limit} parallel")
        
        return max_chunks, parallel_limit
    
    def find_fast_break_points(self, html_content):
        break_points = [0]
        
        essential_patterns = [
            r'</p>\s*<p',
            r'</div>\s*<div',
            r'</h[1-6]>\s*<',
            r'</li>\s*<li',
            r'</tr>\s*<tr'
        ]
        
        for pattern in essential_patterns:
            for match in re.finditer(pattern, html_content, re.IGNORECASE):
                break_point = match.end() - len(match.group().split('<')[-1]) - 1
                if break_point > 0:
                    break_points.append(break_point)
        
        break_points.append(len(html_content))
        return sorted(list(set(break_points)))
    
    def create_fast_chunks(self, html_content, req_logger):
        content_length = len(html_content)
        max_chunks, parallel_limit = self.calculate_optimal_config(content_length, req_logger)
        
        if content_length <= self.target_chars:
            req_logger.log("Single chunk: Content small enough for one request")
            return [{'id': 0, 'content': html_content}], parallel_limit
        
        break_points = self.find_fast_break_points(html_content)
        ideal_chunks = min(max(2, content_length // self.target_chars), max_chunks)
        
        req_logger.log(f"Creating {ideal_chunks} large chunks for speed")
        
        chunks = []
        chars_per_chunk = content_length // ideal_chunks
        current_start = 0
        
        for chunk_id in range(ideal_chunks):
            if chunk_id == ideal_chunks - 1:
                chunk_content = html_content[current_start:]
                if len(chunk_content) > 12000:
                    req_logger.log(f"Warning: Large final chunk ({len(chunk_content)} chars)")
                chunks.append({'id': chunk_id, 'content': chunk_content})
                req_logger.log(f"Chunk {chunk_id}: {len(chunk_content)} chars (final)")
                break
            
            ideal_end = current_start + chars_per_chunk
            best_break = self.find_nearest_break(break_points, ideal_end, current_start)
            chunk_content = html_content[current_start:best_break]
            
            if len(chunk_content.strip()) > 0:
                chunks.append({'id': chunk_id, 'content': chunk_content})
                req_logger.log(f"Chunk {chunk_id}: {len(chunk_content)} chars")
            
            current_start = best_break
        
        return chunks, parallel_limit
    
    def find_nearest_break(self, break_points, target_position, min_position):
        candidates = [bp for bp in break_points if min_position < bp <= target_position + 1000]
        return min(candidates, key=lambda x: abs(x - target_position)) if candidates else target_position

chunker = FastHTMLChunker()

@app.post("/api/translate")
async def translate_content(request: TranslationRequest, http_request: Request):
    request_id = str(uuid.uuid4())[:8]
    req_logger = RequestLogger(request_id)
    
    try:
        req_logger.log(f"ðŸš€ FAST MODE | {request.sourceLanguage} â†’ {request.targetLanguage} | {len(request.content):,} chars")
        
        client = get_openai_client()
        if not client:
            raise HTTPException(status_code=500, detail="OpenAI client not available")
        
        chunks, parallel_limit = chunker.create_fast_chunks(request.content, req_logger)
        
        translated_chunks = await translate_html_chunks_aggressive(
            chunks, request.sourceLanguage, request.targetLanguage, 
            client, parallel_limit, req_logger
        )
        
        final_html = ''.join([chunk['translated_content'] for chunk in sorted(translated_chunks, key=lambda x: x['id'])])
        final_html = clean_translated_html(request.content, final_html)
        
        summary = req_logger.get_summary()
        req_logger.log(f"âš¡ COMPLETE | {summary['total_processing_time']}s | {summary['chunks_successful']}/{len(chunks)} success")
        
        return {
            "translatedContent": final_html,
            "requestId": request_id,
            "processingStats": {
                "totalTime": summary['total_processing_time'],
                "chunksProcessed": len(chunks),
                "parallelWorkers": parallel_limit,
                "successful": summary['chunks_successful']
            },
            "fromCache": False
        }
        
    except Exception as e:
        req_logger.log(f"âŒ FAILED: {str(e)}", "ERROR")
        raise HTTPException(status_code=500, detail=f"Translation failed: {str(e)}")

async def translate_html_chunks_aggressive(chunks, source_lang, target_lang, client, parallel_limit, req_logger):
    semaphore = asyncio.Semaphore(parallel_limit)
    req_logger.log(f"âš¡ AGGRESSIVE MODE: {len(chunks)} chunks, {parallel_limit} workers")
    
    async def translate_chunk_fast(chunk):
        async with semaphore:
            start_time = time.time()
            try:
                content = chunk['content'].strip()
                if not content:
                    req_logger.log(f"Chunk {chunk['id']}: Empty content, skipping")
                    return {'id': chunk['id'], 'translated_content': '', 'success': True}
                
                if len(content) > 12000:
                    req_logger.log(f"Chunk {chunk['id']}: Truncating large content ({len(content)} chars)")
                    content = content[:12000] + "..."
                
                system_prompt = f"Translate HTML from {source_lang} to {target_lang}. Keep ALL HTML tags unchanged. Translate only text content. Return identical structure."
                
                response = await client.chat.completions.create(
                    model="gpt-3.5-turbo-0125",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": content}
                    ],
                    temperature=0.1,
                    max_tokens=4096,
                    presence_penalty=0,
                    frequency_penalty=0
                )
                
                translated_content = response.choices[0].message.content.strip()
                cleaned_content = clean_translated_html(chunk['content'], translated_content)
                
                processing_time = time.time() - start_time
                req_logger.log_chunk_result(chunk['id'], True, len(chunk['content']), processing_time)
                
                return {
                    'id': chunk['id'],
                    'translated_content': cleaned_content,
                    'success': True
                }
                
            except Exception as e:
                processing_time = time.time() - start_time
                req_logger.log_chunk_result(chunk['id'], False, len(chunk['content']), processing_time, str(e))
                
                return {
                    'id': chunk['id'],
                    'translated_content': chunk['content'],
                    'success': False
                }
    
    tasks = [translate_chunk_fast(chunk) for chunk in chunks]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    final_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            req_logger.log(f"Exception in chunk {i}: {str(result)}", "ERROR")
            final_results.append({
                'id': i,
                'translated_content': chunks[i]['content'],
                'success': False
            })
        else:
            final_results.append(result)
    
    successful = len([r for r in final_results if r['success']])
    req_logger.log(f"âš¡ Parallel complete: {successful}/{len(chunks)} successful")
    
    return final_results

@app.get("/health")
async def health_check():
    return {
        "status": "healthy", 
        "version": "3.0.0-fast",
        "openai_ready": bool(get_openai_client())
    }



if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
